# Modelo Seq2Seq con AtenciÃ³n para TraducciÃ³n AutomÃ¡tica (EspaÃ±ol â†” InglÃ©s)

Este repositorio implementa un **modelo de traducciÃ³n automÃ¡tica basado en redes neuronales recurrentes (GRU)** con un **mecanismo de atenciÃ³n**, siguiendo una arquitectura Encoderâ€“Decoder.  
El proyecto carga un dataset paralelo, prepara vocabularios, construye tensores de entrenamiento, entrena el modelo y finalmente **genera traducciones con visualizaciÃ³n de atenciÃ³n**.

---

## ğŸš€ CaracterÃ­sticas del proyecto

- Limpieza y normalizaciÃ³n del texto (remociÃ³n de acentos y caracteres no deseados)
- ConstrucciÃ³n de vocabularios para ambos idiomas
- TokenizaciÃ³n con `SOS`, `EOS` y `PAD`
- CreaciÃ³n de un `Dataset` y `DataLoader` personalizado
- Arquitectura:
  - **Encoder GRU**
  - **Decoder GRU con atenciÃ³n**
- Entrenamiento paso a paso con:
  - *Teacher forcing implÃ­cito mediante uso de predicciones recurrentes*
  - CÃ¡lculo de pÃ©rdida token por token
- PredicciÃ³n de traducciones
- VisualizaciÃ³n de mapas de atenciÃ³n con `matplotlib`

---

ğŸ§© Requisitos

Antes de ejecutar el script, instala las dependencias:

pip install -r requirements.txt

ğŸ§‘â€ğŸ’» Autor

Desarrollado por Gus como parte de su aprendizaje en Python e IA.
